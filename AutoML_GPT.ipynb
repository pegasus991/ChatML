{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai==0.28\n",
    "# !pip install --upgrade ipywidgets\n",
    "# !pip install --upgrade jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To develop machine learning models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Vectorisation of database\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "#Langchain framework to tune the OpenAI API to our documentation\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "#To help maintain and manipulate json objects\n",
    "import json\n",
    "\n",
    "#To display the conversation between the user and chatbot\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#To manipulate the operating system\n",
    "import os\n",
    "\n",
    "#To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the OpenAPI Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"INSERT-API-KEY\"\n",
    "\n",
    "#Defining our LLM configuration \n",
    "llm = OpenAI(model_name = \"gpt-3.5-turbo\", temperature=0, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our vectorised dataset using QdrantClient\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "def load_docs():\n",
    "    print(\"inside\")\n",
    "    directory = 'D:\\Projects\\ChatML\\Model_documents'\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(dirpath, filename), 'r', encoding='utf-8') as file:\n",
    "                    documents = file.readlines()\n",
    "                documents = [doc.strip() for doc in documents]\n",
    "                client.add(collection_name=\"knowledge-base\", documents=documents)\n",
    "                \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to automate model training and metrics generation\n",
    "def runner(json_file, model_file):\n",
    "    # Load parameters from JSON\n",
    "    directory = r'D:\\Projects\\ChatML\\JSONs'\n",
    "    with open(directory+json_file, 'r') as file:\n",
    "        parameters = json.load(file)\n",
    "        \n",
    "    with open(directory+model_file, 'r') as file:\n",
    "        model_param = json.load(file)\n",
    "\n",
    "    model_name = parameters['model_name']\n",
    "    df = pd.read_csv(r\"D:\\Projects\\ChatML\\datasets\" + \"\\\\\" + str(parameters['filename']))\n",
    "    flag = parameters['flag']\n",
    "\n",
    "    #Check for target_variable is present or not\n",
    "    target_variable = parameters.get(\"target_variable\", None)\n",
    "    if target_variable is None:\n",
    "        raise ValueError(\"Target variable not specified in the parameters.\")\n",
    "\n",
    "    X = df.drop(columns=[target_variable])\n",
    "    y = df[target_variable]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=parameters['split'], random_state=42)\n",
    "\n",
    "    def_param = {\n",
    "      \"decision_tree\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
    "      \"svm\" : {\"param_dict\" : \"default_svm_parameters\", \"lib_name\" : \"SVC\"},\n",
    "      \"logistic_regression\" : {\"param_dict\" : \"default_lr_parameters\", \"lib_name\" : \"LogisticRegression\"}\n",
    "    }\n",
    "\n",
    "    params= def_param[model_name][\"param_dict\"]\n",
    "    param = model_param[params]\n",
    "    lib_name = def_param[model_name][\"lib_name\"]\n",
    "    \n",
    "    #NO hyperparameter tuning\n",
    "    if flag == 0:\n",
    "      # Merge default and user-provided parameters\n",
    "        merged_parameters = {**eval(str(param)), **parameters.get(\"param\", {})}\n",
    "      # print(merged_parameters)\n",
    "\n",
    "      # Initialize machine learning model with the merged parameters\n",
    "        model = eval(lib_name)(**merged_parameters)\n",
    "\n",
    "      # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        para = parameters.get(\"param\", {})\n",
    "\n",
    "      # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        cr = classification_report(y_test,y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        return_dict = {\"acc\" : acc, \"cr\" : cr, \"cm\" : cm ,\"paramters\" : str(para)}\n",
    "        return return_dict\n",
    "    \n",
    "    #Hyperparameter tuning\n",
    "    else:\n",
    "        param_grid = parameters.get(\"param\", {})\n",
    "        grid_search = GridSearchCV(eval(lib_name)(), param_grid, cv=2, scoring='accuracy')\n",
    "\n",
    "        # Fit the grid search to the data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best parameters \n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Use the best parameters to train the final model\n",
    "        final_model = grid_search.best_estimator_\n",
    "        \n",
    "        y_pred = final_model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        cr = classification_report(y_test,y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        return_dict = {\"acc\" : acc, \"cr\" : cr, \"cm\" : cm ,\"paramters\" : str(best_params)}\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to stip json file of excess garbage string values\n",
    "def parse_json_garbage(s):\n",
    "    start_idx = next(idx for idx, c in enumerate(s) if c in \"{[\")\n",
    "    s = s[start_idx:]\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return json.loads(s[:e.pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define prompt template using langchain\n",
    "def rag(chat_history: list[str], question: str, n_points: int = 3) -> str:\n",
    "    results = client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "    \n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "    context = context.strip()\n",
    "    \n",
    "    # Define the prompt template with input variables\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"question\", \"chat_history\", \"context\"],\n",
    "        template=\"\"\"\n",
    "    You are a helpful machine learning bot.\n",
    "    Answer the following question using the provided context.\n",
    "    In the beginning of the conversation user will specify which model it wants to use. return 1 when this kind of query is specified. if the model name is not specified in the beginning ask the user and then return 1. \n",
    "    If the user asks you to build a model : If the parameters are not defined by the user, ask them to specify, a sample json file that is to be to made to run\n",
    "    the model looks like this:\n",
    "    [\n",
    "    \"filename\" : \"<filename>.csv\",\n",
    "    \"model_name\" : \"<model_name>\",\n",
    "    \"param\": [\n",
    "        \"<param name>\": \"<value>\"\n",
    "    ],\n",
    "    \"target_variable\": \"<target_name>\",\n",
    "    \"split\" : 0.2,\n",
    "    \"flag\" : 0/1\n",
    "    ] \n",
    "    \n",
    "    Please note that \"flag\" must be 0 by default, unless the user wants to perform \"hyperparameter tuning\" or when the user provides multiple values for some parameters (i.e, when values > 1 for a parameter), only then must be changed to 1.\n",
    "    Also keep in mind that if the user wants to do hyperparameter tuning, i.e., flag = 1; store all the values for a parameter in a python list [], for example:\n",
    "    \n",
    "    \"param\": [\n",
    "        \"<param name>\": [<value1>, <value2>, <value3>]\n",
    "    ]\n",
    "    \n",
    "    This is not the default json. this is just a format, change all the names here according to the latest chat. replace [] with curly brackets.\n",
    "    if everything is mentioned return the json file\n",
    "    if user says build or run or train the model or something of that sort stating to run the given json, return -1\n",
    "    if the user asks for decision tree as model name input in the json file as \"decsion_tree\"\n",
    "    if the user asks for lr as model name input it in the json file as \"logistic_regression\"\n",
    "    if the user asks for support vector machine as model name  in the the input,portray it in the json file as \"svm\"\n",
    "    Always make sure you are checking this first before giving any response. \n",
    "    Also refer to the chat history while answering a question. consider info given by the assisstant only as truth.\n",
    "    The user could ask you two types of questions - one regarding the machine learning parameters and their acceptable values or anything related to the architecture of the machine learning model. The other set of the questions would be related to creating a json file for given parameter values and when user asks you to run the model. When asked direct questions about json building and model running remember answers in the chat history and when asked factual question about the model architecture refer to the context documentation. \n",
    "    If you have limited information on something, state is and then answer \"this is all I know.\" irrespective of how much their word count expectation is.\n",
    "    Also keep in mind that the user can specify multiple model configurations and ask you to store those model parameters and their performance. \n",
    "    For example, a user may say that “design model 1 for decision tree with criterion = entropy, max_depth = 10”, after evaluating performance of model 1 it may provide new parameters like “design model 2 for decision tree with criterion = mini, max_depth = 5”. In such a case, the user may ask you to compare model 1 and model 2 performances which you must do. \n",
    "    \n",
    "\n",
    "    Question: {question}\n",
    "    CHAT HISTORY : {chat_history}\n",
    "    Context: {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create the Langchain instance\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    response = chain.run(question=question, chat_history=chat_history, context = context)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13c658956cc403bb1519ac275aad061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Create an output widget to display chatbot messages\n",
    "output_box = widgets.Output()\n",
    "display(output_box)\n",
    "\n",
    "# Function triggered on text submission\n",
    "def on_submit(input_box):\n",
    "    query = input_box.value\n",
    "    print(f\"User: {query}\")\n",
    "\n",
    "    if query.lower() == 'exit':\n",
    "        with output_box:\n",
    "            display(Markdown(\"**Thank you for using the State of the Union chatbot!**\"))\n",
    "        return \n",
    "    \n",
    "    # Load model documentation if this is the first interaction\n",
    "    if len(chat_history) == 0:\n",
    "        with output_box:\n",
    "            display(Markdown(\"**Chatbot:** Loading the model documentation, please wait...\"))\n",
    "        load_docs()\n",
    "        chat_history.append((query, \"done\"))\n",
    "        with output_box:\n",
    "            display(Markdown(\"**Chatbot:** Machine Learning models documentation loaded into memory!\"))\n",
    "    \n",
    "    # Answer the user's queries\n",
    "    elif len(chat_history) != 0:\n",
    "        response = rag(chat_history, query)\n",
    "        result = response\n",
    "        \n",
    "        if result == '-1':  # Placeholder for handling specific responses\n",
    "            with output_box:\n",
    "                display(Markdown(f\"**User:** {query}\"))\n",
    "                display(Markdown(\"**Chatbot:** Building model, please wait...\"))\n",
    "            chat_history.append((query, 'building model please wait'))\n",
    "            data_dict = parse_json_garbage(chat_history[-2][-1])\n",
    "            print(data_dict)\n",
    "            \n",
    "            json_file_path = \"./JSONs/sample.json\"\n",
    "            with open(json_file_path, 'w') as json_file:\n",
    "                json.dump(data_dict, json_file, indent=2)\n",
    "            \n",
    "            result = runner(\"sample.json\", \"model_parameters.json\")\n",
    "            acc = result['acc']\n",
    "            cr = result['cr']\n",
    "            cm = result['cm']\n",
    "            bp = result['parameters']\n",
    "            \n",
    "            # Append results to chat history and display in output\n",
    "            result = 'The following are the model parameters and evaluation metrics ' + str(result)\n",
    "            chat_history.append((query, result))\n",
    "            \n",
    "            with output_box:\n",
    "                display(Markdown(f\"**Chatbot:** The parameters used are: {bp}\"))\n",
    "                display(Markdown(f\"**Chatbot:** The accuracy of the model is {acc}\"))\n",
    "                display(Markdown(f\"**Chatbot:** The classification report of the model is:\\n\\n{cr}\"))\n",
    "                \n",
    "                # Display confusion matrix\n",
    "                sns.heatmap(cm, annot=True)\n",
    "                plt.show()\n",
    "        \n",
    "        else:\n",
    "            # If there's a general response, add it to chat history and display\n",
    "            chat_history.append((query, result))\n",
    "            with output_box:\n",
    "                display(Markdown(f\"**User:** {query}\"))\n",
    "                display(Markdown(f\"**Chatbot:** {result}\"))\n",
    "    \n",
    "    input_box.value = \"\"  # Clear the input box after submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb2e4c189a147c9b4d506e8b636d34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'done'),\n",
       " ('What is SVM ?',\n",
       "  'SVM stands for Support Vector Machine. It is a supervised machine learning algorithm used for classification and regression tasks. In the context of sklearn, SVC (Support Vector Classifier) is used for implementing SVM for classification tasks.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ChatML)",
   "language": "python",
   "name": "chatml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
